name: Release

on:
  workflow_dispatch:  # Manual trigger from GitHub UI
    inputs:
      version:
        description: 'Version (e.g., 1.0.0)'
        required: true
        type: string
  push:
    tags:
      - 'v*'  # Triggers on version tags like v1.0.0

permissions:
  contents: write

jobs:
  # Build Python package (cross-platform, requires Python)
  build-python:
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install PyTorch (CPU for export)
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install onnx onnxruntime onnxscript
      
      - name: Download Model Weights
        run: |
          curl -L -o inference/marvin_small.pt https://huggingface.co/holymolyyy/marvin/resolve/main/marvin_small.pt
        shell: bash
      
      - name: Export ONNX model
        run: |
          python scripts/export_onnx.py --device cpu
      
      - name: Determine version
        id: version
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "version=${{ inputs.version }}" >> $GITHUB_OUTPUT
          else
            # Extract from tag (v1.0.0 -> 1.0.0)
            echo "version=${GITHUB_REF_NAME#v}" >> $GITHUB_OUTPUT
          fi
      
      - name: Build release package
        run: |
          python scripts/build_release.py --version ${{ steps.version.outputs.version }}
      
      - name: Upload Python package
        uses: actions/upload-artifact@v4
        with:
          name: marvin-onnx-python-v${{ steps.version.outputs.version }}
          path: dist/marvin-onnx-v${{ steps.version.outputs.version }}.zip

  # Build Windows executable (standalone, no Python needed)
  build-windows-exe:
    runs-on: windows-latest
    needs: build-python
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install onnx onnxruntime onnxscript pyinstaller
      
      - name: Download Model Weights
        run: |
          curl -L -o inference/marvin_small.pt https://huggingface.co/holymolyyy/marvin/resolve/main/marvin_small.pt
        shell: bash
      
      - name: Export ONNX model
        run: python scripts/export_onnx.py --device cpu
        env:
          PYTHONIOENCODING: utf-8
      
      - name: Build executable
        run: python scripts/build_executable.py --version ${{ needs.build-python.outputs.version }} --no-zip
      
      - name: Create Windows archive
        run: |
          Compress-Archive -Path dist/marvin-onnx/* -DestinationPath dist/marvin-onnx-windows-v${{ needs.build-python.outputs.version }}.zip
        shell: pwsh
      
      - name: Upload Windows executable
        uses: actions/upload-artifact@v4
        with:
          name: marvin-onnx-windows-v${{ needs.build-python.outputs.version }}
          path: dist/marvin-onnx-windows-v${{ needs.build-python.outputs.version }}.zip

  # Build Linux executable (standalone, no Python needed)
  build-linux-exe:
    runs-on: ubuntu-latest
    needs: build-python
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
          pip install -r requirements.txt
          pip install onnx onnxruntime onnxscript pyinstaller
      
      - name: Download Model Weights
        run: |
          curl -L -o inference/marvin_small.pt https://huggingface.co/holymolyyy/marvin/resolve/main/marvin_small.pt
        shell: bash
      
      - name: Export ONNX model
        run: python scripts/export_onnx.py --device cpu
      
      - name: Build executable
        run: python scripts/build_executable.py --version ${{ needs.build-python.outputs.version }} --no-zip
      
      - name: Create Linux archive
        run: |
          cd dist && tar -czvf marvin-onnx-linux-v${{ needs.build-python.outputs.version }}.tar.gz marvin-onnx/
      
      - name: Upload Linux executable
        uses: actions/upload-artifact@v4
        with:
          name: marvin-onnx-linux-v${{ needs.build-python.outputs.version }}
          path: dist/marvin-onnx-linux-v${{ needs.build-python.outputs.version }}.tar.gz

  # Create GitHub Release
  release:
    runs-on: ubuntu-latest
    needs: [build-python, build-windows-exe, build-linux-exe]
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      
      - name: List artifacts
        run: find artifacts -type f
      
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: |
            artifacts/**/*.zip
            artifacts/**/*.tar.gz
          body: |
            ## Marvin Chess AI v${{ needs.build-python.outputs.version }}
            
            A human-like chess engine that mimics play styles across skill levels (1200-2400 Elo).
            
            > **Note**: This release uses ONNX Runtime for **CPU inference only**. GPU acceleration is not included in the standalone executables. For GPU support, use the PyTorch-based engine from the source repository.
            
            ### Downloads
            
            | Platform | File | Requirements |
            |----------|------|--------------|
            | **Windows (standalone)** | `marvin-onnx-windows-v${{ needs.build-python.outputs.version }}.zip` | None! Just extract and run |
            | **Linux (standalone)** | `marvin-onnx-linux-v${{ needs.build-python.outputs.version }}.tar.gz` | None! Just extract and run |
            | **Cross-platform (Python)** | `marvin-onnx-python-v${{ needs.build-python.outputs.version }}.zip` | Python 3.10+ |
            
            ### Quick Start (Standalone)
            
            1. Download the file for your platform
            2. Extract to a folder
            3. Configure your chess GUI to use:
               - Windows: `marvin-onnx.exe`
               - Linux: `./marvin-onnx`
            
            ### Quick Start (Python)
            
            1. Download the Python package
            2. Extract and run: `pip install -r requirements.txt`
            3. Run: `python uci_onnx.py`
            
            ### UCI Options
            
            #### Sampling & Style
            | Option | Type | Default | Description |
            |--------|------|---------|-------------|
            | Temperature | string | 0.9 | Move sampling temperature. Lower = more deterministic, higher = more random |
            | TopP | string | 0.95 | Nucleus sampling threshold. Only considers moves in top P probability mass |
            | TimeTemperature | string | 0.5 | Temperature for time prediction sampling |
            | TimeTopP | string | 0.95 | Top-P for time prediction |
            | OpeningTemperature | string | 1.2 | Higher temperature in opening for variety |
            | OpeningLength | spin | 10 | Number of plies to use opening temperature (0-100) |
            
            #### Elo & Ratings
            | Option | Type | Default | Description |
            |--------|------|---------|-------------|
            | HumanElo | spin | 2400 | Simulated opponent Elo for model conditioning (1200-2400) |
            | EngineElo | spin | 2400 | Engine's target playing strength (1200-2400) |
            
            #### Time Management
            | Option | Type | Default | Description |
            |--------|------|---------|-------------|
            | UseModeTime | check | false | Use mode (most likely) time instead of sampling |
            | UseExpectedTime | check | true | Use expected value of time distribution |
            | UseRealTime | check | false | Use actual inference time as think time |
            | SimulateThinkingTime | check | false | Add artificial delay to simulate human thinking |
            | InternalClock | check | false | Track time internally (for engines without clock info) |
            | DebugClocks | check | false | Print clock debug info to stderr |
            | GameBaseTime | string | 0 | Base time in seconds for internal clock |
            | GameIncrement | string | 0 | Increment in seconds for internal clock |
            
            #### MCTS (Monte Carlo Tree Search)
            | Option | Type | Default | Description |
            |--------|------|---------|-------------|
            | UseMCTS | check | false | Enable MCTS search |
            | MCTSSimulations | spin | 256 | Number of MCTS simulations (1-200000) |
            | MCTSCpuct | string | 2.0 | Exploration constant (higher = more exploration) |
            | MCTSMaxChildren | spin | 48 | Max children per node (1-4096) |
            | MCTSRootDirichletAlpha | string | 0.0 | Dirichlet noise alpha at root (0 = disabled) |
            | MCTSRootExplorationFrac | string | 0.0 | Fraction of exploration noise at root |
            | MCTSFinalTemperature | string | 0.0 | Temperature for final move selection (0 = argmax) |
            | MCTSFinalTopP | string | 0.9 | Top-P for final move selection |
            | MCTSMaxDepth | spin | 96 | Maximum search depth (1-512) |
            | MCTSAdaptive | check | true | Scale simulations based on model's predicted think time Ã— MCTSAdaptiveScale |
            | MCTSAdaptiveScale | string | 150.0 | Multiplier for predicted time to determine simulation count |
            | MCTSFPU | string | 0.2 | First Play Urgency - value for unexplored nodes |
            | MCTSContempt | string | 0.15 | Draw aversion (positive = avoid draws) |
            | MCTSSimulateTime | check | false | Simulate think time in MCTS |
            | MCTSStartPly | spin | 0 | Ply to start using MCTS (0-100) |
            | MCTSTreeReuse | check | true | Reuse search tree between moves |
            | LogMctsStats | check | false | Log MCTS statistics to stderr |
            
            ### Links
            
            - [Documentation](https://github.com/zingalorp/marvin-chess)
            - [Play on Lichess](https://lichess.org/@/marvin-2000)
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
